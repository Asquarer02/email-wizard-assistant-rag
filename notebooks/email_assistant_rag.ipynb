{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b72145a",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5215a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import time\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d32398",
   "metadata": {},
   "source": [
    "# Load Environment Variables (for OpenAI API Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"OPENAI_API_KEY not found in .env file. Please set it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50fb3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/proc_email.csv\" \n",
    "FAISS_INDEX_PATH = \"../faiss_index\" \n",
    "EMAIL_COUNT = 60 \n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 20\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-3-small\" \n",
    "LLM_MODEL_NAME = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e74d6",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "## Made use of first 60 emails from the Enron Emails dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1984469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: ../data/proc_email.csv\n",
      "Loaded 99 total emails.\n",
      "Selected 60 emails for processing.\n",
      "\n",
      "Sample document content (from 'to_index' column):\n",
      "To: frozenset({'robert.walker@enron.com'})\n",
      "From: frozenset({'daren.farmer@enron.com'})\n",
      "X-To: Robert Walker\n",
      "X-From: Daren J Farmer\n",
      "content: ENA Contact\n",
      "\n",
      "Daren Farmer\n",
      "Phone # 713-853-6905\n",
      "Fax# 713-646-2391\n",
      "\n",
      "EB3211F\n",
      "to_index: From Daren J Farmer to Robert Walker: ENA Contact\n",
      "\n",
      "Daren Farmer\n",
      "Phone # 713-853-6905\n",
      "Fax# 713-646-2391\n",
      "\n",
      "EB3211F...\n",
      "\n",
      "Sample document metadata (source): From Daren J Farmer to Robert Walker: ENA Contact\n",
      "\n",
      "Daren Farmer\n",
      "Phone # 713-853-6905\n",
      "Fax# 713-646-23...\n",
      "\n",
      "--- Text Splitting ---\n",
      "Split 60 documents into 188 chunks.\n",
      "Sample split chunk: To: frozenset({'robert.walker@enron.com'})\n",
      "From: frozenset({'daren.farmer@enron.com'})\n",
      "X-To: Robert Walker\n",
      "X-From: Daren J Farmer\n",
      "content: ENA Contact\n",
      "\n",
      "Daren Farmer\n",
      "Phone # 713-853-6905\n",
      "Fax# 713-646-2...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading data from: {DATA_PATH}\")\n",
    "\n",
    "loader = CSVLoader(file_path=DATA_PATH,\n",
    "                   encoding=\"utf8\",\n",
    "                   source_column=\"to_index\") \n",
    "\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} total emails.\") \n",
    "\n",
    "if len(documents) >= EMAIL_COUNT:\n",
    "    selected_documents = documents[:EMAIL_COUNT]\n",
    "    print(f\"Selected {len(selected_documents)} emails for processing.\")\n",
    "else:\n",
    "    selected_documents = documents\n",
    "    print(f\"Warning: Fewer than {EMAIL_COUNT} emails available. Using all {len(selected_documents)} loaded emails.\")\n",
    "\n",
    "if selected_documents:\n",
    "    print(\"\\nSample document content (from 'to_index' column):\")\n",
    "    \n",
    "    print(selected_documents[0].page_content[:500] + \"...\")\n",
    "    print(f\"\\nSample document metadata (source): {selected_documents[0].metadata['source'][:100]}...\")\n",
    "else:\n",
    "    print(\"No documents were loaded or selected. Exiting.\")\n",
    "    \n",
    "    \n",
    "print(\"\\n--- Text Splitting ---\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "all_splits = text_splitter.split_documents(selected_documents)\n",
    "\n",
    "print(f\"Split {len(selected_documents)} documents into {len(all_splits)} chunks.\")\n",
    "if all_splits:\n",
    "    print(f\"Sample split chunk: {all_splits[0].page_content[:200]}...\")\n",
    "else:\n",
    "    print(\"No splits created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3105d03",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "## Created embedding vectors of the emails using OpenAI Embeddings and stored them in FAISS VectorDB for similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa42aabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized OpenAIEmbeddings with model: text-embedding-3-small\n",
      "Creating new FAISS index...\n",
      "FAISS index created and saved to: ../faiss_index\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)\n",
    "print(f\"Initialized OpenAIEmbeddings with model: {EMBEDDING_MODEL_NAME}\")\n",
    "\n",
    "if os.path.exists(FAISS_INDEX_PATH) and os.listdir(FAISS_INDEX_PATH):\n",
    "    print(f\"Loading existing FAISS index from: {FAISS_INDEX_PATH}\")\n",
    "    try:\n",
    "        vector_store = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "        print(\"FAISS index loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FAISS index: {e}. Recreating index.\")\n",
    "        if all_splits:\n",
    "            print(\"Creating new FAISS index...\")\n",
    "            vector_store = FAISS.from_documents(documents=all_splits, embedding=embeddings)\n",
    "            vector_store.save_local(FAISS_INDEX_PATH)\n",
    "            print(f\"FAISS index created and saved to: {FAISS_INDEX_PATH}\")\n",
    "        else:\n",
    "            print(\"No document splits to create index from. Cannot proceed.\")\n",
    "            \n",
    "else:\n",
    "    if all_splits:\n",
    "        print(\"Creating new FAISS index...\")\n",
    "        vector_store = FAISS.from_documents(documents=all_splits, embedding=embeddings)\n",
    "        os.makedirs(FAISS_INDEX_PATH, exist_ok=True) \n",
    "        vector_store.save_local(FAISS_INDEX_PATH)\n",
    "        print(f\"FAISS index created and saved to: {FAISS_INDEX_PATH}\")\n",
    "    else:\n",
    "        print(\"No document splits to create index from. Cannot proceed with FAISS creation.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547bab79",
   "metadata": {},
   "source": [
    "# Retriever and QA Chain Setup\n",
    "## Initialised a QA Chain with 4o-mini along with prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bb750ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever created from FAISS vector store (for QA chain).\n",
      "\n",
      "Testing FAISS similarity_search_with_score with query: 'What is the ENA contact information for Daren Farmer?'\n",
      "Retrieved 3 documents with scores.\n",
      "--- Document 1 (Score: 0.6297) ---\n",
      "To: frozenset({'robert.walker@enron.com'})\n",
      "From: frozenset({'daren.farmer@enron.com'})\n",
      "X-To: Robert Walker\n",
      "X-From: Daren J Farmer\n",
      "content: ENA Contact\n",
      "\n",
      "Daren Farmer\n",
      "Phone # 713-853-6905\n",
      "Fax# 713-646-2391\n",
      "\n",
      "EB3211F\n",
      "to_index: From Daren J Farmer to Robert Walker: ENA Contact\n",
      "\n",
      "Daren Farmer\n",
      "Phone # 713-8...\n",
      "(Source: From Daren J Farmer to Robert Walker: ENA Contact\n",
      "\n",
      "Daren Farmer\n",
      "Phone # 713-853-6905\n",
      "Fax# 713-646-23...)\n",
      "--------------------\n",
      "--- Document 2 (Score: 0.9362) ---\n",
      "Group-\n",
      "\n",
      "Earlier this week, we met Tana Jones (Senior Legal Specialist) and Leslie \n",
      "Hansen (Legal Counsel) with ENA (servicing ENW) to discuss the process for \n",
      "executing NDAs.  Leslie will serve as in-house legal counsel for EMS while \n",
      "Tana will be the Senior Legal Assistant for all documentation suc...\n",
      "(Source: From Tana Jones to Stephen Abbanat: ----- Forwarded by Tana Jones/HOU/ECT on 10/26/2000 02:22 PM ---...)\n",
      "--------------------\n",
      "--- Document 3 (Score: 0.9362) ---\n",
      "Group-\n",
      "\n",
      "Earlier this week, we met Tana Jones (Senior Legal Specialist) and Leslie \n",
      "Hansen (Legal Counsel) with ENA (servicing ENW) to discuss the process for \n",
      "executing NDAs.  Leslie will serve as in-house legal counsel for EMS while \n",
      "Tana will be the Senior Legal Assistant for all documentation suc...\n",
      "(Source: From Tana Jones to Stephen Abbanat: ----- Forwarded by Tana Jones/HOU/ECT on 10/26/2000 02:22 PM ---...)\n",
      "--------------------\n",
      "\n",
      "Initialized ChatOpenAI with model: gpt-4o-mini\n",
      "RAG QA chain created.\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3}) \n",
    "print(\"Retriever created from FAISS vector store (for QA chain).\")\n",
    "sample_query_for_similarity_search = \"What is the ENA contact information for Daren Farmer?\"\n",
    "print(f\"\\nTesting FAISS similarity_search_with_score with query: '{sample_query_for_similarity_search}'\")\n",
    "docs_and_scores = vector_store.similarity_search_with_score(\n",
    "    query=sample_query_for_similarity_search,\n",
    "    k=3 \n",
    ")\n",
    "\n",
    "print(f\"Retrieved {len(docs_and_scores)} documents with scores.\")\n",
    "for i, (doc, score) in enumerate(docs_and_scores):\n",
    "    print(f\"--- Document {i+1} (Score: {score:.4f}) ---\") \n",
    "    print(doc.page_content[:300] + \"...\")\n",
    "    print(f\"(Source: {doc.metadata.get('source', 'N/A')[:100]}...)\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "llm = ChatOpenAI(model_name=LLM_MODEL_NAME, temperature=0.7)\n",
    "print(f\"\\nInitialized ChatOpenAI with model: {LLM_MODEL_NAME}\")\n",
    "\n",
    "prompt_template = \"\"\"You are an Email Wizard's Assistant. Use the following pieces of context, which are past emails, to answer the question at the end.\n",
    "If you don't know the answer from the context, just say that you don't know, don't try to make up an answer.\n",
    "Provide a concise answer, and if possible, quote relevant parts from the retrieved emails.\n",
    "If the question is a general greeting or not answerable from the emails, respond politely.\n",
    "Context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever, \n",
    "    return_source_documents=True, \n",
    "    chain_type_kwargs={\"prompt\": QA_PROMPT}\n",
    ")\n",
    "print(\"RAG QA chain created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829eecfc",
   "metadata": {},
   "source": [
    "# Testing RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "745f91c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Query: What is the phone number for Daren Farmer at ENA?\n",
      "\n",
      "LLM Answer:\n",
      "The phone number for Daren Farmer at ENA is 713-853-6905.\n",
      "\n",
      "\n",
      "Query: What is the discussion about i2 Technologies and Tax?\n",
      "\n",
      "LLM Answer:\n",
      "The discussion about i2 Technologies and Tax revolves around the approval of contracts related to Enron Credit.com. Tana Jones is seeking confirmation that Erica's legal advice, which expresses no problem with the agreement, should apply to not just the Non-Disclosure Agreement (NDA) in question, but also to all NDAs and the entire business originating from the Houston-based EnronCredit.com team. Tana states, \"Erica's advice needs to apply not only to this NDA, but probably all of the NDA's that are going to originate from the Houston based EnronCredit.com business team.\" Additionally, Mark Taylor emphasizes the need to confirm whether Tax has no issues with these contracts being signed in the U.S.\n",
      "\n",
      "\n",
      "Query: Who should be contacted about the MDEA Agreement scheduling?\n",
      "\n",
      "LLM Answer:\n",
      "The contact for the MDEA Agreement scheduling should be Bob Priest, as he is mentioned as available for a meeting early next week. Reagan Rorschach states, \"Bob is only available Monday,\" indicating he is a key person in this scheduling. Additionally, Marvin is also involved in the discussions.\n",
      "\n",
      "\n",
      "Query: Tell me about the Vitro/Termination agreement.\n",
      "\n",
      "LLM Answer:\n",
      "The context does not provide specific details about the Vitro/Termination agreement itself. However, it indicates that Kay Mann has not seen a signed copy of the termination agreement and is concerned about ensuring that the fully executed document has been processed. Kay mentioned in an email, \"I just want to make sure that the fully executed document has been processed.\" Additionally, Peggy Banczak requested copies of any GE contracts related to the agreement, stating, \"I have not received copies of any of the GE contracts.\" \n",
      "\n",
      "For more specific information about the contents or implications of the Vitro/Termination agreement, there is no available data in the emails.\n",
      "\n",
      "\n",
      "Query: What's the status of my project?\n",
      "\n",
      "LLM Answer:\n",
      "I'm sorry, but the context provided does not include any specific information about the status of your project.\n",
      "\n",
      "\n",
      "Query: Hi, how are you?\n",
      "\n",
      "LLM Answer:\n",
      "I'm sorry, but I don't have that information from the emails.\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "        \"What is the phone number for Daren Farmer at ENA?\",\n",
    "        \"What is the discussion about i2 Technologies and Tax?\",\n",
    "        \"Who should be contacted about the MDEA Agreement scheduling?\",\n",
    "        \"Tell me about the Vitro/Termination agreement.\",\n",
    "        \"What's the status of my project?\", \n",
    "        \"Hi, how are you?\" \n",
    "    ]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n\\nQuery: {query}\")\n",
    "    try:\n",
    "        result = qa_chain.invoke({\"query\": query})\n",
    "        print(\"\\nLLM Answer:\")\n",
    "        print(result[\"result\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query '{query}': {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e119a61d",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "## Calculated inference times and similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63ca9a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Processing Query 1/6 ---\n",
      "Query: What is the phone number for Daren Farmer at ENA?\n",
      "\n",
      "  Retrieval - Similarity Scores:\n",
      "    Retrieval time: 1.2577 sec\n",
      "    Doc 1 - Similarity Score : 0.6766\n",
      "    Doc 2 - Similarity Score : 0.9760\n",
      "    Doc 3 - Similarity Score : 0.9761\n",
      "\n",
      "  RAG System - LLM Response:\n",
      "    Full RAG Inference time: 6.3441 sec.\n",
      "\n",
      "    LLM Answer:\n",
      "    The phone number for Daren Farmer at ENA is 713-853-6905.\n",
      "\n",
      "\n",
      "--- Processing Query 2/6 ---\n",
      "Query: What is the discussion about i2 Technologies and Tax?\n",
      "\n",
      "  Retrieval - Similarity Scores:\n",
      "    Retrieval time: 0.6112 sec\n",
      "    Doc 1 - Similarity Score : 0.9415\n",
      "    Doc 2 - Similarity Score : 1.0458\n",
      "    Doc 3 - Similarity Score : 1.1927\n",
      "\n",
      "  RAG System - LLM Response:\n",
      "    Full RAG Inference time: 3.5868 sec.\n",
      "\n",
      "    LLM Answer:\n",
      "    The discussion about i2 Technologies and Tax revolves around the approval of contracts associated with Enron Credit.com. Erica Gut expressed her approval of the agreement but raised concerns about how Texas law would be applied by English courts. Tana Jones suggested that Erica's advice should apply broadly, not just to a specific NDA, but to all NDAs and the overall business originating from the Houston-based EnronCredit.com team. Tana stated, \"It seems to me Erica's advice needs to apply not only to this NDA, but probably all of the NDA's that are going to originate from the Houston based EnronCredit.com business team.\"\n",
      "\n",
      "\n",
      "--- Processing Query 3/6 ---\n",
      "Query: Who should be contacted about the MDEA Agreement scheduling?\n",
      "\n",
      "  Retrieval - Similarity Scores:\n",
      "    Retrieval time: 0.5119 sec\n",
      "    Doc 1 - Similarity Score : 0.8944\n",
      "    Doc 2 - Similarity Score : 0.9115\n",
      "    Doc 3 - Similarity Score : 1.0805\n",
      "\n",
      "  RAG System - LLM Response:\n",
      "    Full RAG Inference time: 2.2518 sec.\n",
      "\n",
      "    LLM Answer:\n",
      "    The MDEA Agreement scheduling should involve contacting Bob Priest and Marvin. Reagan Rorschach mentioned, \"I'd like to get together with Bob and Marvin early next week to discuss the agreement.\"\n",
      "\n",
      "\n",
      "--- Processing Query 4/6 ---\n",
      "Query: Tell me about the Vitro/Termination agreement.\n",
      "\n",
      "  Retrieval - Similarity Scores:\n",
      "    Retrieval time: 0.8988 sec\n",
      "    Doc 1 - Similarity Score : 0.8657\n",
      "    Doc 2 - Similarity Score : 0.9162\n",
      "    Doc 3 - Similarity Score : 1.0734\n",
      "\n",
      "  RAG System - LLM Response:\n",
      "    Full RAG Inference time: 4.9622 sec.\n",
      "\n",
      "    LLM Answer:\n",
      "    The emails indicate that there is a discussion about a termination agreement related to Vitro. Kay Mann mentioned, \"I haven't seen a signed copy of the termination agreement cross my desk,\" indicating uncertainty about the document's status. Peggy Banczak inquired about the GE contracts and asked for \"a copy of whatever has been executed.\" This suggests that the termination agreement is part of a broader set of contracts that are under review, but specific details about the content of the Vitro/Termination agreement itself are not provided in the emails.\n",
      "\n",
      "\n",
      "--- Processing Query 5/6 ---\n",
      "Query: What's the status of my project?\n",
      "\n",
      "  Retrieval - Similarity Scores:\n",
      "    Retrieval time: 0.6692 sec\n",
      "    Doc 1 - Similarity Score : 1.4451\n",
      "    Doc 2 - Similarity Score : 1.4615\n",
      "    Doc 3 - Similarity Score : 1.4723\n",
      "\n",
      "  RAG System - LLM Response:\n",
      "    Full RAG Inference time: 3.0728 sec.\n",
      "\n",
      "    LLM Answer:\n",
      "    The status of your project is that Kay Mann is still waiting on David Hunt to send some material that would help move the document along. She mentioned that there are still internal questions annotated in the document and expressed a need for definitive commercial direction from you. Additionally, she inquired if the commercial team has finished with the exhibits, suggesting they may need work. Kay also mentioned that early next week is preferable for her schedule, as Thursday and Friday are significant problem days for her.\n",
      "\n",
      "\n",
      "--- Processing Query 6/6 ---\n",
      "Query: Hi, how are you?\n",
      "\n",
      "  Retrieval - Similarity Scores:\n",
      "    Retrieval time: 0.8154 sec\n",
      "    Doc 1 - Similarity Score : 1.3444\n",
      "    Doc 2 - Similarity Score : 1.4882\n",
      "    Doc 3 - Similarity Score : 1.5040\n",
      "\n",
      "  RAG System - LLM Response:\n",
      "    Full RAG Inference time: 2.0420 sec.\n",
      "\n",
      "    LLM Answer:\n",
      "    I'm just a program, but thank you for asking! If you have any specific questions regarding the emails, feel free to ask.\n",
      "\n",
      "\n",
      "--- Overall Performance Summary ---\n",
      "Average Full RAG Inference Time over 6 queries: 3.7099 sec.\n"
     ]
    }
   ],
   "source": [
    "all_inference_times = [] \n",
    "K_FOR_SCORES = 3 \n",
    "\n",
    "for query_idx, query in enumerate(queries):\n",
    "    print(f\"\\n\\n--- Processing Query {query_idx + 1}/{len(queries)} ---\")\n",
    "    print(f\"Query: {query}\")\n",
    "\n",
    "    \n",
    "    print(\"\\n  Retrieval - Similarity Scores:\")\n",
    "    try:\n",
    "        retrieval_start_time = time.time()\n",
    "        \n",
    "        docs_and_scores = vector_store.similarity_search_with_score(query, k=K_FOR_SCORES)\n",
    "        retrieval_end_time = time.time()\n",
    "        retrieval_time = retrieval_end_time - retrieval_start_time\n",
    "        print(f\"    Retrieval time: {retrieval_time:.4f} sec\")\n",
    "\n",
    "        if docs_and_scores:\n",
    "            current_query_scores = [score for doc, score in docs_and_scores]\n",
    "\n",
    "            for i, (doc, score) in enumerate(docs_and_scores):\n",
    "\n",
    "                print(f\"    Doc {i+1} - Similarity Score : {score:.4f}\")\n",
    "        else:\n",
    "            print(\"No documents retrieved by similarity search.\")\n",
    "            \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    Error during similarity search for query '{query}': {e}\")\n",
    "    \n",
    "    print(\"\\n  RAG System - LLM Response:\")\n",
    "    try:\n",
    "        rag_start_time = time.time()\n",
    "        result = qa_chain.invoke({\"query\": query})\n",
    "        rag_end_time = time.time()\n",
    "        current_inference_time = rag_end_time - rag_start_time\n",
    "        all_inference_times.append(current_inference_time)\n",
    "        print(f\"    Full RAG Inference time: {current_inference_time:.4f} sec.\")\n",
    "        print(\"\\n    LLM Answer:\")\n",
    "        print(f\"    {result.get('result', 'N/A')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    Error processing RAG chain for query '{query}': {e}\")\n",
    "        all_inference_times.append(np.nan) \n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Overall Performance Summary ---\")\n",
    "\n",
    "if all_inference_times:\n",
    "    valid_inference_times = [t for t in all_inference_times if not np.isnan(t)]\n",
    "    if valid_inference_times:\n",
    "        avg_inference_time = np.mean(valid_inference_times)\n",
    "        print(f\"Average Full RAG Inference Time over {len(valid_inference_times)} queries: {avg_inference_time:.4f} sec.\")\n",
    "    else:\n",
    "        print(\"No valid RAG inference times recorded to calculate an average.\")\n",
    "else:\n",
    "    print(\"No RAG inference times recorded.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
