{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b72145a",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5215a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d32398",
   "metadata": {},
   "source": [
    "# Load Environment Variables (for OpenAI API Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"OPENAI_API_KEY not found in .env file. Please set it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50fb3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/proc_email.csv\" \n",
    "FAISS_INDEX_PATH = \"../faiss_index\" \n",
    "EMAIL_COUNT = 60 \n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 20\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-3-small\" \n",
    "LLM_MODEL_NAME = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e74d6",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1984469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: ../data/proc_email.csv\n",
      "Loaded 99 total emails.\n",
      "Selected 60 emails for processing.\n",
      "\n",
      "Sample document content (from 'to_index' column):\n",
      "To: frozenset({'robert.walker@enron.com'})\n",
      "From: frozenset({'daren.farmer@enron.com'})\n",
      "X-To: Robert Walker\n",
      "X-From: Daren J Farmer\n",
      "content: ENA Contact\n",
      "\n",
      "Daren Farmer\n",
      "Phone # 713-853-6905\n",
      "Fax# 713-646-2391\n",
      "\n",
      "EB3211F\n",
      "to_index: From Daren J Farmer to Robert Walker: ENA Contact\n",
      "\n",
      "Daren Farmer\n",
      "Phone # 713-853-6905\n",
      "Fax# 713-646-2391\n",
      "\n",
      "EB3211F...\n",
      "\n",
      "Sample document metadata (source): From Daren J Farmer to Robert Walker: ENA Contact\n",
      "\n",
      "Daren Farmer\n",
      "Phone # 713-853-6905\n",
      "Fax# 713-646-23...\n",
      "\n",
      "--- Text Splitting ---\n",
      "Split 60 documents into 188 chunks.\n",
      "Sample split chunk: To: frozenset({'robert.walker@enron.com'})\n",
      "From: frozenset({'daren.farmer@enron.com'})\n",
      "X-To: Robert Walker\n",
      "X-From: Daren J Farmer\n",
      "content: ENA Contact\n",
      "\n",
      "Daren Farmer\n",
      "Phone # 713-853-6905\n",
      "Fax# 713-646-2...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading data from: {DATA_PATH}\")\n",
    "\n",
    "loader = CSVLoader(file_path=DATA_PATH,\n",
    "                   encoding=\"utf8\",\n",
    "                   source_column=\"to_index\") \n",
    "\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} total emails.\") \n",
    "\n",
    "if len(documents) >= EMAIL_COUNT:\n",
    "    selected_documents = documents[:EMAIL_COUNT]\n",
    "    print(f\"Selected {len(selected_documents)} emails for processing.\")\n",
    "else:\n",
    "    selected_documents = documents\n",
    "    print(f\"Warning: Fewer than {EMAIL_COUNT} emails available. Using all {len(selected_documents)} loaded emails.\")\n",
    "\n",
    "if selected_documents:\n",
    "    print(\"\\nSample document content (from 'to_index' column):\")\n",
    "    \n",
    "    print(selected_documents[0].page_content[:500] + \"...\")\n",
    "    print(f\"\\nSample document metadata (source): {selected_documents[0].metadata['source'][:100]}...\")\n",
    "else:\n",
    "    print(\"No documents were loaded or selected. Exiting.\")\n",
    "    \n",
    "    \n",
    "print(\"\\n--- Text Splitting ---\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "all_splits = text_splitter.split_documents(selected_documents)\n",
    "\n",
    "print(f\"Split {len(selected_documents)} documents into {len(all_splits)} chunks.\")\n",
    "if all_splits:\n",
    "    print(f\"Sample split chunk: {all_splits[0].page_content[:200]}...\")\n",
    "else:\n",
    "    print(\"No splits created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3105d03",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa42aabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized OpenAIEmbeddings with model: text-embedding-3-small\n",
      "Creating new FAISS index...\n",
      "FAISS index created and saved to: ../faiss_index\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)\n",
    "print(f\"Initialized OpenAIEmbeddings with model: {EMBEDDING_MODEL_NAME}\")\n",
    "\n",
    "if os.path.exists(FAISS_INDEX_PATH) and os.listdir(FAISS_INDEX_PATH):\n",
    "    print(f\"Loading existing FAISS index from: {FAISS_INDEX_PATH}\")\n",
    "    try:\n",
    "        vector_store = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "        print(\"FAISS index loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FAISS index: {e}. Recreating index.\")\n",
    "        if all_splits:\n",
    "            print(\"Creating new FAISS index...\")\n",
    "            vector_store = FAISS.from_documents(documents=all_splits, embedding=embeddings)\n",
    "            vector_store.save_local(FAISS_INDEX_PATH)\n",
    "            print(f\"FAISS index created and saved to: {FAISS_INDEX_PATH}\")\n",
    "        else:\n",
    "            print(\"No document splits to create index from. Cannot proceed.\")\n",
    "            \n",
    "else:\n",
    "    if all_splits:\n",
    "        print(\"Creating new FAISS index...\")\n",
    "        vector_store = FAISS.from_documents(documents=all_splits, embedding=embeddings)\n",
    "        os.makedirs(FAISS_INDEX_PATH, exist_ok=True) \n",
    "        vector_store.save_local(FAISS_INDEX_PATH)\n",
    "        print(f\"FAISS index created and saved to: {FAISS_INDEX_PATH}\")\n",
    "    else:\n",
    "        print(\"No document splits to create index from. Cannot proceed with FAISS creation.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecae47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
